# Inside the DINOv2 Architecture

<head>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css">
  <script src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"></script>
</head>

<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/j2_42Yx_1_w" frameborder="0" allowfullscreen></iframe>
</div>


## Table of Contents

* [Acknowledgment](#acknowledgment)
* [References](#references)
* [DINOv2 architecture](#dinov2-architecture)
  * [global_crops and local_crops](#global_crops-and-local_crops)
  * [Masking out global_crops patches](#masking-out-global_crops-patches)
  * [ViT backbone](#vit-backbone)
  * [dino head](#dino-head)
  * [ibot head](#ibot-head)
  * [dino loss](#dino-loss)
  * [ibot loss](#ibot-loss)
  * [weights update](#weights-update)
  * [putting it all together](#putting-it-all-together)
* [Testing DINOv2 on CIFAR-10](#testing-dinov2-on-cifar-10)
  * [Importing libraries and preparing the dataset](#importing-libraries-and-preparing-the-dataset)
  * [Downloading the DINO model and getting the features out](#downloading-the-dino-model-and-getting-the-features-out)
  * [Training a logistic regression classifier](#training-a-logistic-regression-classifier)
  * [Test results](#test-results)


## Acknowledgment:
I borrowed some code from [DINOv2 repository](https://github.com/facebookresearch/dinov2).

## References:
```bibtex
@inproceedings{caron2021emerging,
  title     = {Emerging Properties in Self-Supervised Vision Transformers},
  author    = {Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J{\'e}gou, Herv{\'e} and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  booktitle = {Proceedings of the International Conference on Computer Vision (ICCV)},
  year      = {2021}
}
```
```bibtex
@article{zhou2021ibot,
  title     = {iBOT: Image BERT Pre-Training with Online Tokenizer},
  author    = {Zhou, Jinghao and Wei, Chen and Wang, Huiyu and Shen, Wei and Xie, Cihang and Yuille, Alan and Kong, Tao},
  journal   = {International Conference on Learning Representations (ICLR)},
  year      = {2022}
}
```

```bibtex
@misc{oquab2023dinov2,
  title   = {DINOv2: Learning Robust Visual Features without Supervision},
  author  = {Oquab, Maxime and Darcet, Timoth√©e and Moutakanni, Theo and Vo, Huy V. and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and Howes, Russell and Huang, Po-Yao and Xu, Hu and Sharma, Vasu and Li, Shang-Wen and Galuba, Wojciech and Rabbat, Mike and Assran, Mido and Ballas, Nicolas and Synnaeve, Gabriel and Misra, Ishan and Jegou, Herve and Mairal, Julien and Labatut, Patrick and Joulin, Armand and Bojanowski, Piotr},
  journal = {arXiv:2304.07193},
  year    = {2023}
}
```

## DINOv2 architecture

### `global_crops` and `local_crops`

![DINOv2-001](https://github.com/user-attachments/assets/f37626ae-2210-491e-a54f-15fd3596c429)

---

![DINOv2-002](https://github.com/user-attachments/assets/a84c537b-527e-47b7-91fb-7f90de0fdd4f)


### Masking out `global_crops` patches

![DINOv2-003](https://github.com/user-attachments/assets/16016557-d686-4533-b23a-d9ebe1bc1535)

---

### ViT backbone

![DINOv2-004](https://github.com/user-attachments/assets/7a8a502f-87d8-4046-a053-7102fbff162e)

---

![DINOv2-005](https://github.com/user-attachments/assets/023a4021-895b-4a35-81c3-e9301d76a801)

### dino head

![DINOv2-006](https://github.com/user-attachments/assets/2bbde612-7516-42d4-9368-9a4a042426ea)

### ibot head

![DINOv2-008](https://github.com/user-attachments/assets/c93a4ca6-466f-4511-8c42-cc43bdbcbce2)

### dino loss

![DINOv2-007](https://github.com/user-attachments/assets/34c96238-a203-4471-8ac1-85734ea0073b)

### ibot loss

![DINOv2-009](https://github.com/user-attachments/assets/8901a118-6aff-4171-96a7-3c529b12d4d6)

### weights update

The student network is updated with standard SGD, while the teacher network uses an exponential moving average (EMA) according to the following update rule:

![Untitled001](https://github.com/user-attachments/assets/252ced6d-aa2c-49a5-af91-61aca4b8db5c)

### putting it all together

![drawings-02 005](https://github.com/user-attachments/assets/61e19647-b6f1-4b8b-b1f9-c02071a5e935)


## Testing DINOv2 on CIFAR-10

### Importing libraries and preparing the dataset
```python
import torch
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import numpy as np
import random
import os
from sklearn.linear_model import LogisticRegression
```

```python
Config = {
    'NUM_CLASSES': 10,
    'BATCH_SIZE': 128,
    'NUM_EPOCHS': 100,
    'LR': 0.001,
    'WIDTH': 224,
    'HEIGHT': 224,
    'DATA_MEANS': np.array([0.49139968, 0.48215841, 0.44653091]), # mean of the CIFAR dataset, used for normalization
    'DATA_STD': np.array([0.24703223, 0.24348513, 0.26158784]),   # standard deviation of the CIFAR dataset, used for normalization
    'CROP_SCALES': (0.8, 1.0),
    'CROP_RATIO': (0.9, 1.1),
    'SEED': 42,
}

# Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class_names = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')
```

```python
# Load CIFAR-10 dataset
transform = transforms.Compose([
    transforms.Resize((Config['HEIGHT'], Config['WIDTH'])),  # DINOv2 requires 224x224 input
    transforms.ToTensor(),
    transforms.Normalize(Config['DATA_MEANS'], Config['DATA_STD'])])

trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                        download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=Config['BATCH_SIZE'], shuffle=True, num_workers=2)

testset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                       download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=Config['BATCH_SIZE'], shuffle=False, num_workers=2)
```

```python
# Number of images to display
num_images = 10

# Create a figure and axes
fig, axes = plt.subplots(1, num_images, figsize=(20, 5))

# Select 10 random indices
random_indices = random.sample(range(len(trainloader.dataset)), num_images)

# Iterate through the random images and display them
for i, index in enumerate(random_indices):
    img, label = trainloader.dataset[index]

    # Unnormalize (reverse the normalization for display)
    mean = torch.tensor(Config['DATA_MEANS']).view(3, 1, 1)
    std = torch.tensor(Config['DATA_STD']).view(3, 1, 1)
    img = img * std + mean  # Undo normalization
    img = np.transpose(img.numpy(), (1, 2, 0))  # Convert from CHW to HWC

    axes[i].imshow(np.clip(img, 0, 1))
    axes[i].set_title(class_names[label])
    axes[i].axis('off')

plt.tight_layout()
plt.show()
```

![image](https://github.com/user-attachments/assets/08355809-3d4e-4bd7-a830-6f7412a81aa4)

### Downloading the DINO model and getting the features out

```python
# DINOv2
dinov2_vits14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14').to(device)
dinov2_vits14.eval()  # Set to evaluation mode (no fine-tuning)
```

```console
Using cache found in /root/.cache/torch/hub/facebookresearch_dinov2_main
DinoVisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 384, kernel_size=(14, 14), stride=(14, 14))
    (norm): Identity()
  )
  (blocks): ModuleList(
    (0-11): 12 x NestedTensorBlock(
      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (attn): MemEffAttention(
        (qkv): Linear(in_features=384, out_features=1152, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=384, out_features=384, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (ls1): LayerScale()
      (drop_path1): Identity()
      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU(approximate='none')
        (fc2): Linear(in_features=1536, out_features=384, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (ls2): LayerScale()
      (drop_path2): Identity()
    )
  )
  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (head): Identity()
)
```

```python
# Function to extract features using DINOv2
def extract_features(model, dataloader):
    features = []
    labels = []
    with torch.no_grad():
        for images, lbls in dataloader:
            images = images.to(device)
            feats = model(images)  # Shape: (batch_size, 384) for dinov2_vits14
            features.append(feats.cpu().numpy())
            labels.append(lbls.numpy())
    return np.concatenate(features), np.concatenate(labels)
```

```
# Training features: Load if exists, extract if not
if os.path.exists("train_features.npy") and os.path.exists("train_labels.npy"):
    print("Loading precomputed training features...")
    train_features = np.load("train_features.npy")
    train_labels = np.load("train_labels.npy")
else:
    print("Extracting training features...")
    train_features, train_labels = extract_features(dinov2_vits14, trainloader, save_path="train")

# Test features: Load if exists, extract if not
if os.path.exists("test_features.npy") and os.path.exists("test_labels.npy"):
    print("Loading precomputed test features...")
    test_features = np.load("test_features.npy")
    test_labels = np.load("test_labels.npy")
else:
    print("Extracting test features...")
    test_features, test_labels = extract_features(dinov2_vits14, testloader, save_path="test")
```

### Training a logistic regression classifier

```python
# Train a simple classifier (logistic regression) on the features
classifier = LogisticRegression(max_iter=1000)
classifier.fit(train_features, train_labels)

# Evaluate the classifier
train_accuracy = classifier.score(train_features, train_labels)
test_accuracy = classifier.score(test_features, test_labels)
print(f"Training accuracy: {train_accuracy:.4f}")
print(f"Test accuracy: {test_accuracy:.4f}")
```

```console
Training accuracy: 0.9832
Test accuracy: 0.9501
```

```python
# Updated classify_image to display image with labels
def classify_image(image_tensor, actual_label, model, classifier, class_names):
    # Get prediction
    img = image_tensor.unsqueeze(0).to(device)
    with torch.no_grad():
        feature = model(img).cpu().numpy()
    pred = classifier.predict(feature)
    pred_class = class_names[pred[0]]
    actual_class = class_names[actual_label]

    # Unnormalize image for display
    mean = torch.tensor(Config['DATA_MEANS']).view(3, 1, 1)
    std = torch.tensor(Config['DATA_STD']).view(3, 1, 1)
    img_display = image_tensor * std + mean  # Undo normalization
    img_display = img_display.numpy().transpose(1, 2, 0)  # CHW to HWC
    img_display = np.clip(img_display, 0, 1)  # Ensure valid range

    # Display image with title
    plt.figure(figsize=(4, 4))
    plt.imshow(img_display)
    plt.title(f"Actual: {actual_class}\nPredicted: {pred_class}", fontsize=12)
    plt.axis('off')
    plt.show()

    return pred_class

# Test on a random test image
test_image, test_label = testset[random.randint(0, len(testset))]
pred_class = classify_image(test_image, test_label, dinov2_vits14, classifier, class_names)
```

### Test results

While the classifier achieves 95% test accuracy, I'm pulling out some of its incorrect predictions to understand what confuses the classifier.

![DINOv2-CIFAR10-wrong-predict-005](https://github.com/user-attachments/assets/054b7bbe-5103-49f8-8cfa-76dd6b737e64)
![DINOv2-CIFAR10-wrong-predict-004](https://github.com/user-attachments/assets/6f5e88de-99ec-41d3-a3a6-2d3bd9a378ea)
![DINOv2-CIFAR10-wrong-predict-003](https://github.com/user-attachments/assets/85cfb72c-6e6e-4b7c-9d5f-6f50c468c906)
![DINOv2-CIFAR10-wrong-predict-002](https://github.com/user-attachments/assets/b31c5e70-b705-4fcc-9607-f49bdcc89cc6)
![DINOv2-CIFAR10-wrong-predict-001](https://github.com/user-attachments/assets/ccca16d7-c54e-4ffa-9b14-2ff854114f5e)




<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
        {left: '$$', right: '$$', display: true}, // Display math (e.g., equations on their own line)
        {left: '$', right: '$', display: false},  // Inline math (e.g., within a sentence)
        {left: '\\(', right: '\\)', display: false}, // Another way to write inline math
        {left: '\\[', right: '\\]', display: true}   // Another way to write display math
      ]
    });
  });
</script>

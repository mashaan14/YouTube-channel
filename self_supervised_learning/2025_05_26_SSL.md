# Self-Supervised Learning Review: from SimCLR to DINOv2

<head>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css">
  <script src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"></script>
</head>

<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/G6c6zk0RhRM" frameborder="0" allowfullscreen></iframe>
</div>

## Acknowledgment:
I borrowed some code from [DINOv2 repository](https://github.com/facebookresearch/dinov2).

## References:
```bibtex
@inproceedings{caron2021emerging,
  title     = {Emerging Properties in Self-Supervised Vision Transformers},
  author    = {Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J{\'e}gou, Herv{\'e} and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  booktitle = {Proceedings of the International Conference on Computer Vision (ICCV)},
  year      = {2021}
}
```

![drawings-01 001](https://github.com/user-attachments/assets/db07ce41-fff7-41fa-a648-186d4d8cf9af)


## SimCLR

![drawings-02 001](https://github.com/user-attachments/assets/c9c19cde-0bac-43ba-be8c-dc1508615a18)

---

![drawings 004](https://github.com/user-attachments/assets/be97b241-8429-4f1f-8230-ba1db6aec673)

---

![drawings 005](https://github.com/user-attachments/assets/77304cbd-1369-4a29-b773-43f0c139ea5b)

---

![drawings 006](https://github.com/user-attachments/assets/fa5b86ae-7cc6-43c2-b66d-d5df9227da8a)

## BYOL

![drawings-02 002](https://github.com/user-attachments/assets/2e2e32a7-67e9-4ff1-a795-650273c8e3a6)

## SwAV

SwAV added a layer to cluster the images. Their motivation was to avoid the costly pairwise assignment of positive and negative pairs.

> Comparing cluster assignments allows to contrast different image views while not relying on explicit pairwise feature comparisons.
>
> source: Caron et al., 2020

![drawings-02 003](https://github.com/user-attachments/assets/f38b09c6-1960-4c9f-9757-2b711a5071bc)

---

SwAV uses multi-crop training, where the image is cropped into smaller sizes.

> In this work, we propose multi-crop that uses smaller-sized images to increase the number of views while not increasing the memory or computational requirements during training.
>
> source: Caron et al., 2020

![drawings-01 005](https://github.com/user-attachments/assets/a9cc28c0-f000-4be4-b639-9a8e1fcfead6)


## DINOv1

DINOv1 utilized the power of vision transformers and replaced the ConvNet backbone used in previous works.

> In this work, inspired from these methods, we study the impact of self-supervised pretraining on ViT features. Of particular interest, we have identified several interesting properties that do not emerge with supervised ViTs, nor with convnets:
> * Self-supervised ViT features explicitly contain the scene layout and, in particular, object boundaries, as shown in Figure 1. This information is directly accessible in the self-attention modules of the last block.
> * Self-supervised ViT features perform particularly well with a basic nearest neighbors classifier (k-NN) without any finetuning, linear classifier nor data augmentation, achieving 78.3% top-1 accuracy on ImageNet.
>
> source: Caron et al., 2021

![drawings-02 004](https://github.com/user-attachments/assets/b2777775-3c6e-4925-bd56-f4aff743ec90)

---

![Untitled 002](https://github.com/user-attachments/assets/e3d7d9b2-220f-445b-97cb-c944027ca967)
> source: Caron et al., 2021

---

![Untitled 001](https://github.com/user-attachments/assets/5fa6cae6-f3c2-400e-86b5-55ec3ac1c606)
> source: Caron et al., 2021


## iBOT

iBOT paper introduces the concept of masked patch token and designed a loss for this.

> The target network is fed with a masked image while the online tokenizer with the original image. The goal is to let the target network recover each masked patch token to its corresponding tokenizer output. Our online tokenizer naturally resolves two major challenges.
> * On the one hand, our tokenizer captures highlevel visual semantics progressively learned by enforcing the similarity of cross-view images on class tokens.
> * On the other hand, our tokenizer needs no extra stages of training as pre-processing setup since it is jointly optimized with MIM via momentum update.
>   
> source: Zhou et al., 2022


![drawings-02 005](https://github.com/user-attachments/assets/c7f16023-eb8d-4774-90af-d58d5f3806e2)



<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
      delimiters: [
        {left: '$$', right: '$$', display: true}, // Display math (e.g., equations on their own line)
        {left: '$', right: '$', display: false},  // Inline math (e.g., within a sentence)
        {left: '\\(', right: '\\)', display: false}, // Another way to write inline math
        {left: '\\[', right: '\\]', display: true}   // Another way to write display math
      ]
    });
  });
</script>
